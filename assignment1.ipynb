{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR100\n",
    "# from torchvision.models.resnet import resnet18\n",
    "# from torchvision.models.vgg import vgg11\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Define transform and load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "        transforms.Pad(4, padding_mode='reflect'),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "            np.array([63.0, 62.1, 66.7]) / 255.0),\n",
    "    ])\n",
    "val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            np.array([125.3, 123.0, 113.9]) / 255.0,\n",
    "            np.array([63.0, 62.1, 66.7]) / 255.0),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR100(root='cifar100', train=True, download=True, transform=train_transform)\n",
    "valid_dataset = CIFAR100(root='cifar100', train=False, download=True, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=128,\n",
    "                              shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                            batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = resnet18(pretrained=False, num_classes=100)\n",
    "# vgg = vgg11(pretrained=False, num_classes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Define the training and test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    #滑动平均\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    #topk准确率\n",
    "    #预测结果前k个中出现的正确结果的次数\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_missing(directory):\n",
    "    #创建文件夹，如果这个文件夹不存在的话\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best=False, fpath=''):\n",
    "    if len(osp.dirname(fpath)) != 0:\n",
    "        mkdir_if_missing(osp.dirname(fpath))\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        shutil.copy(fpath, osp.join(osp.dirname(fpath), 'best_model.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
    "    lr = initial_lr\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_tqdm(data_loader, disable_tqdm):\n",
    "    #进度条打印\n",
    "    if disable_tqdm:\n",
    "        tqdm_loader = data_loader\n",
    "    else:\n",
    "        tqdm_loader = tqdm(data_loader, ncols=0)\n",
    "    return tqdm_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    #每个epoch的优化过程\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for input, target in warp_tqdm(train_loader, True):\n",
    "\n",
    "\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "\n",
    "    log = 'Epoch:{0}\\tLoss: {loss.avg:.4f}\\t'.format(epoch, loss=losses)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for input, target in test_loader:\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(input)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(output.data, target)[0]\n",
    "        top1.update(acc1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "\n",
    "    log = 'Test Acc@1: {top1.avg:.3f}'.format(top1=top1)\n",
    "\n",
    "    return top1.avg, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train a VGG/ResNet on CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = 'experiments/cifar100-vgg'\n",
    "# num_epochs = 20\n",
    "# model = vgg.cuda()\n",
    "# train_loader = train_loader\n",
    "# test_loader = valid_loader\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# best_acc = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "#     train_log = train(train_loader, model, criterion, optimizer, epoch)\n",
    "#     acc, test_log = test(test_loader, model, criterion)\n",
    "#     log = train_log + test_log\n",
    "#     print(log)\n",
    "#     is_best = acc > best_acc\n",
    "#     best_acc = max(acc, best_acc)\n",
    "#     if is_best:\n",
    "#         save_checkpoint({'epoch':epoch,\n",
    "#         'state_dict':model.state_dict(),\n",
    "#         'acc': acc,\n",
    "#         }, False, 'best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tLoss: 3.8583\tTest Acc@1: 14.830\n",
      "Epoch:1\tLoss: 3.1818\tTest Acc@1: 28.210\n",
      "Epoch:2\tLoss: 2.6438\tTest Acc@1: 34.100\n",
      "Epoch:3\tLoss: 2.2708\tTest Acc@1: 38.120\n",
      "Epoch:4\tLoss: 2.0209\tTest Acc@1: 41.180\n",
      "Epoch:5\tLoss: 1.8594\tTest Acc@1: 44.400\n",
      "Epoch:6\tLoss: 1.7324\tTest Acc@1: 48.650\n",
      "Epoch:7\tLoss: 1.6488\tTest Acc@1: 47.330\n",
      "Epoch:8\tLoss: 1.5713\tTest Acc@1: 52.250\n",
      "Epoch:9\tLoss: 1.5037\tTest Acc@1: 51.740\n",
      "Epoch:10\tLoss: 1.4584\tTest Acc@1: 52.680\n",
      "Epoch:11\tLoss: 1.4232\tTest Acc@1: 53.640\n",
      "Epoch:12\tLoss: 1.3804\tTest Acc@1: 54.320\n",
      "Epoch:13\tLoss: 1.3474\tTest Acc@1: 53.350\n",
      "Epoch:14\tLoss: 1.3332\tTest Acc@1: 53.610\n",
      "Epoch:15\tLoss: 1.2980\tTest Acc@1: 51.560\n",
      "Epoch:16\tLoss: 1.2828\tTest Acc@1: 53.110\n",
      "Epoch:17\tLoss: 1.2602\tTest Acc@1: 53.330\n",
      "Epoch:18\tLoss: 1.2411\tTest Acc@1: 56.370\n",
      "Epoch:19\tLoss: 1.2298\tTest Acc@1: 54.180\n",
      "Epoch:20\tLoss: 1.2091\tTest Acc@1: 56.570\n",
      "Epoch:21\tLoss: 1.1948\tTest Acc@1: 57.200\n",
      "Epoch:22\tLoss: 1.1792\tTest Acc@1: 54.430\n",
      "Epoch:23\tLoss: 1.1723\tTest Acc@1: 56.250\n",
      "Epoch:24\tLoss: 1.1648\tTest Acc@1: 57.620\n",
      "Epoch:25\tLoss: 1.1592\tTest Acc@1: 54.700\n",
      "Epoch:26\tLoss: 1.1387\tTest Acc@1: 57.760\n",
      "Epoch:27\tLoss: 1.1373\tTest Acc@1: 58.110\n",
      "Epoch:28\tLoss: 1.1364\tTest Acc@1: 56.520\n",
      "Epoch:29\tLoss: 1.1259\tTest Acc@1: 57.070\n",
      "Epoch:30\tLoss: 1.1000\tTest Acc@1: 53.540\n",
      "Epoch:31\tLoss: 1.1110\tTest Acc@1: 58.790\n",
      "Epoch:32\tLoss: 1.1079\tTest Acc@1: 57.990\n",
      "Epoch:33\tLoss: 1.0909\tTest Acc@1: 58.710\n",
      "Epoch:34\tLoss: 1.0860\tTest Acc@1: 58.110\n",
      "Epoch:35\tLoss: 1.0827\tTest Acc@1: 60.150\n",
      "Epoch:36\tLoss: 1.0720\tTest Acc@1: 58.800\n",
      "Epoch:37\tLoss: 1.0779\tTest Acc@1: 58.880\n",
      "Epoch:38\tLoss: 1.0714\tTest Acc@1: 57.750\n",
      "Epoch:39\tLoss: 1.0643\tTest Acc@1: 57.770\n",
      "Epoch:40\tLoss: 1.0669\tTest Acc@1: 57.060\n",
      "Epoch:41\tLoss: 1.0622\tTest Acc@1: 58.340\n",
      "Epoch:42\tLoss: 1.0623\tTest Acc@1: 58.510\n",
      "Epoch:43\tLoss: 1.0566\tTest Acc@1: 53.450\n",
      "Epoch:44\tLoss: 1.0597\tTest Acc@1: 60.210\n",
      "Epoch:45\tLoss: 1.0458\tTest Acc@1: 60.150\n",
      "Epoch:46\tLoss: 1.0400\tTest Acc@1: 55.740\n",
      "Epoch:47\tLoss: 1.0482\tTest Acc@1: 56.320\n",
      "Epoch:48\tLoss: 1.0452\tTest Acc@1: 57.020\n",
      "Epoch:49\tLoss: 1.0431\tTest Acc@1: 58.260\n",
      "Epoch:50\tLoss: 1.0366\tTest Acc@1: 53.020\n",
      "Epoch:51\tLoss: 1.0322\tTest Acc@1: 57.090\n",
      "Epoch:52\tLoss: 1.0246\tTest Acc@1: 59.930\n",
      "Epoch:53\tLoss: 1.0403\tTest Acc@1: 49.180\n",
      "Epoch:54\tLoss: 1.0215\tTest Acc@1: 60.770\n",
      "Epoch:55\tLoss: 1.0198\tTest Acc@1: 57.370\n",
      "Epoch:56\tLoss: 1.0191\tTest Acc@1: 59.330\n",
      "Epoch:57\tLoss: 1.0287\tTest Acc@1: 56.850\n",
      "Epoch:58\tLoss: 1.0192\tTest Acc@1: 61.080\n",
      "Epoch:59\tLoss: 1.0147\tTest Acc@1: 57.150\n",
      "Epoch:60\tLoss: 1.0089\tTest Acc@1: 60.570\n",
      "Epoch:61\tLoss: 1.0115\tTest Acc@1: 59.310\n",
      "Epoch:62\tLoss: 1.0169\tTest Acc@1: 60.400\n",
      "Epoch:63\tLoss: 1.0001\tTest Acc@1: 59.060\n",
      "Epoch:64\tLoss: 1.0136\tTest Acc@1: 59.350\n",
      "Epoch:65\tLoss: 1.0053\tTest Acc@1: 61.190\n",
      "Epoch:66\tLoss: 1.0133\tTest Acc@1: 57.280\n",
      "Epoch:67\tLoss: 1.0059\tTest Acc@1: 57.180\n",
      "Epoch:68\tLoss: 1.0129\tTest Acc@1: 59.450\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'experiments/cifar100-resnet'\n",
    "from net.resnet import resnet18\n",
    "num_epochs = 200\n",
    "model = resnet18()\n",
    "model = model.cuda()\n",
    "train_loader = train_loader\n",
    "test_loader = valid_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    scheduler.step()\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False, 'best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tLoss: 4.2144\tTest Acc@1: 13.410\n",
      "Epoch:1\tLoss: 3.9004\tTest Acc@1: 18.950\n",
      "Epoch:2\tLoss: 3.6564\tTest Acc@1: 26.020\n",
      "Epoch:3\tLoss: 3.4454\tTest Acc@1: 29.500\n",
      "Epoch:4\tLoss: 3.2514\tTest Acc@1: 37.400\n",
      "Epoch:5\tLoss: 3.1567\tTest Acc@1: 38.220\n",
      "Epoch:6\tLoss: 3.0594\tTest Acc@1: 44.350\n",
      "Epoch:7\tLoss: 2.9979\tTest Acc@1: 44.610\n",
      "Epoch:8\tLoss: 2.9544\tTest Acc@1: 47.590\n",
      "Epoch:9\tLoss: 2.9104\tTest Acc@1: 45.860\n",
      "Epoch:10\tLoss: 2.8797\tTest Acc@1: 50.530\n",
      "Epoch:11\tLoss: 2.8385\tTest Acc@1: 45.340\n",
      "Epoch:12\tLoss: 2.7911\tTest Acc@1: 49.840\n",
      "Epoch:13\tLoss: 2.8631\tTest Acc@1: 50.610\n",
      "Epoch:14\tLoss: 2.7527\tTest Acc@1: 49.610\n",
      "Epoch:15\tLoss: 2.6976\tTest Acc@1: 48.690\n",
      "Epoch:16\tLoss: 2.7478\tTest Acc@1: 55.840\n",
      "Epoch:17\tLoss: 2.6854\tTest Acc@1: 53.500\n",
      "Epoch:18\tLoss: 2.6543\tTest Acc@1: 52.650\n",
      "Epoch:19\tLoss: 2.7000\tTest Acc@1: 52.600\n",
      "Epoch:20\tLoss: 2.7359\tTest Acc@1: 56.630\n",
      "Epoch:21\tLoss: 2.6685\tTest Acc@1: 47.840\n",
      "Epoch:22\tLoss: 2.6703\tTest Acc@1: 54.160\n",
      "Epoch:23\tLoss: 2.6225\tTest Acc@1: 54.940\n",
      "Epoch:24\tLoss: 2.6414\tTest Acc@1: 57.340\n",
      "Epoch:25\tLoss: 2.6363\tTest Acc@1: 57.170\n",
      "Epoch:26\tLoss: 2.5997\tTest Acc@1: 51.060\n",
      "Epoch:27\tLoss: 2.6470\tTest Acc@1: 55.330\n",
      "Epoch:28\tLoss: 2.6634\tTest Acc@1: 56.050\n",
      "Epoch:29\tLoss: 2.6046\tTest Acc@1: 56.470\n",
      "Epoch:30\tLoss: 2.6040\tTest Acc@1: 56.350\n",
      "Epoch:31\tLoss: 2.5898\tTest Acc@1: 52.880\n",
      "Epoch:32\tLoss: 2.6010\tTest Acc@1: 54.500\n",
      "Epoch:33\tLoss: 2.6106\tTest Acc@1: 52.990\n",
      "Epoch:34\tLoss: 2.5491\tTest Acc@1: 56.130\n",
      "Epoch:35\tLoss: 2.5734\tTest Acc@1: 54.520\n",
      "Epoch:36\tLoss: 2.5995\tTest Acc@1: 56.330\n",
      "Epoch:37\tLoss: 2.5894\tTest Acc@1: 55.760\n",
      "Epoch:38\tLoss: 2.5343\tTest Acc@1: 55.950\n",
      "Epoch:39\tLoss: 2.5281\tTest Acc@1: 52.730\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'experiments/cifar100-resnet'\n",
    "from net.resnet import resnet18\n",
    "from utils.mixup import mixup_train\n",
    "num_epochs = 200\n",
    "model = resnet18()\n",
    "model = model.cuda()\n",
    "train_loader = train_loader\n",
    "test_loader = valid_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = mixup_train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    scheduler.step()\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False, 'best_model_mixup.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tLoss: 4.3495\tTest Acc@1: 10.050\n",
      "Epoch:1\tLoss: 4.1585\tTest Acc@1: 13.900\n",
      "Epoch:2\tLoss: 4.0223\tTest Acc@1: 20.220\n",
      "Epoch:3\tLoss: 3.8658\tTest Acc@1: 25.000\n",
      "Epoch:4\tLoss: 3.7475\tTest Acc@1: 27.970\n",
      "Epoch:5\tLoss: 3.5747\tTest Acc@1: 32.770\n",
      "Epoch:6\tLoss: 3.5111\tTest Acc@1: 34.850\n",
      "Epoch:7\tLoss: 3.4100\tTest Acc@1: 37.640\n",
      "Epoch:8\tLoss: 3.3665\tTest Acc@1: 33.860\n",
      "Epoch:9\tLoss: 3.2639\tTest Acc@1: 37.720\n",
      "Epoch:10\tLoss: 3.2944\tTest Acc@1: 39.580\n",
      "Epoch:11\tLoss: 3.2121\tTest Acc@1: 43.450\n",
      "Epoch:12\tLoss: 3.2199\tTest Acc@1: 43.710\n",
      "Epoch:13\tLoss: 3.2031\tTest Acc@1: 39.850\n",
      "Epoch:14\tLoss: 3.2083\tTest Acc@1: 41.100\n",
      "Epoch:15\tLoss: 3.1384\tTest Acc@1: 44.240\n",
      "Epoch:16\tLoss: 3.1084\tTest Acc@1: 46.220\n",
      "Epoch:17\tLoss: 3.0481\tTest Acc@1: 49.240\n",
      "Epoch:18\tLoss: 3.0962\tTest Acc@1: 50.030\n",
      "Epoch:19\tLoss: 3.0489\tTest Acc@1: 42.140\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'experiments/cifar100-resnet'\n",
    "from net.resnet import resnet18\n",
    "from utils.cutmix import cutmix_train\n",
    "num_epochs = 200\n",
    "model = resnet18()\n",
    "model = model.cuda()\n",
    "train_loader = train_loader\n",
    "test_loader = valid_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = cutmix_train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    scheduler.step()\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False, 'best_model_cutmix.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\tLoss: 4.0402\tTest Acc@1: 11.440\n",
      "Epoch:1\tLoss: 3.6340\tTest Acc@1: 14.950\n",
      "Epoch:2\tLoss: 3.3421\tTest Acc@1: 22.740\n",
      "Epoch:3\tLoss: 3.1401\tTest Acc@1: 23.040\n",
      "Epoch:4\tLoss: 2.9621\tTest Acc@1: 26.450\n",
      "Epoch:5\tLoss: 2.7842\tTest Acc@1: 32.350\n",
      "Epoch:6\tLoss: 2.6508\tTest Acc@1: 38.380\n",
      "Epoch:7\tLoss: 2.5202\tTest Acc@1: 41.150\n",
      "Epoch:8\tLoss: 2.4383\tTest Acc@1: 42.460\n",
      "Epoch:9\tLoss: 2.3333\tTest Acc@1: 40.370\n",
      "Epoch:10\tLoss: 2.2727\tTest Acc@1: 39.440\n",
      "Epoch:11\tLoss: 2.2335\tTest Acc@1: 43.400\n",
      "Epoch:12\tLoss: 2.2066\tTest Acc@1: 41.010\n",
      "Epoch:13\tLoss: 2.1160\tTest Acc@1: 48.520\n",
      "Epoch:14\tLoss: 2.1084\tTest Acc@1: 42.260\n",
      "Epoch:15\tLoss: 2.0905\tTest Acc@1: 49.340\n",
      "Epoch:16\tLoss: 2.0541\tTest Acc@1: 47.080\n",
      "Epoch:17\tLoss: 2.0652\tTest Acc@1: 48.430\n",
      "Epoch:18\tLoss: 2.0231\tTest Acc@1: 49.460\n",
      "Epoch:19\tLoss: 2.0214\tTest Acc@1: 49.890\n",
      "Epoch:20\tLoss: 1.9714\tTest Acc@1: 49.440\n",
      "Epoch:21\tLoss: 1.9611\tTest Acc@1: 52.340\n",
      "Epoch:22\tLoss: 1.9502\tTest Acc@1: 50.590\n",
      "Epoch:23\tLoss: 1.9938\tTest Acc@1: 51.540\n",
      "Epoch:24\tLoss: 1.9740\tTest Acc@1: 47.460\n",
      "Epoch:25\tLoss: 1.8767\tTest Acc@1: 44.090\n",
      "Epoch:26\tLoss: 1.9479\tTest Acc@1: 52.380\n",
      "Epoch:27\tLoss: 1.9145\tTest Acc@1: 41.900\n",
      "Epoch:28\tLoss: 1.9006\tTest Acc@1: 49.510\n",
      "Epoch:29\tLoss: 1.9105\tTest Acc@1: 53.630\n",
      "Epoch:30\tLoss: 1.8677\tTest Acc@1: 52.160\n",
      "Epoch:31\tLoss: 1.8983\tTest Acc@1: 49.530\n",
      "Epoch:32\tLoss: 1.8625\tTest Acc@1: 46.920\n",
      "Epoch:33\tLoss: 1.8785\tTest Acc@1: 54.650\n",
      "Epoch:34\tLoss: 1.8903\tTest Acc@1: 48.490\n",
      "Epoch:35\tLoss: 1.8764\tTest Acc@1: 49.330\n",
      "Epoch:36\tLoss: 1.8585\tTest Acc@1: 51.370\n",
      "Epoch:37\tLoss: 1.8631\tTest Acc@1: 51.380\n",
      "Epoch:38\tLoss: 1.8397\tTest Acc@1: 48.990\n",
      "Epoch:39\tLoss: 1.8462\tTest Acc@1: 53.550\n"
     ]
    }
   ],
   "source": [
    "ckpt = 'experiments/cifar100-resnet'\n",
    "from net.resnet import resnet18\n",
    "from utils.cutout import cutout_train\n",
    "num_epochs = 200\n",
    "model = resnet18()\n",
    "model = model.cuda()\n",
    "train_loader = train_loader\n",
    "test_loader = valid_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = cutout_train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    scheduler.step()\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False, 'best_model_cutout.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5c4e78ddba005cc6d98c60f47080c0f1ab65713302979afccc4f234e09c8e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('dl10': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
